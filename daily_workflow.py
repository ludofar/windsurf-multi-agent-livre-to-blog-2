#!/usr/bin/env python3
"""
Workflow quotidien automatis√© pour la g√©n√©ration de contenu MTC.

Ce script g√®re le processus complet de traitement de contenu :
1. V√©rification des nouveaux PDF √† traiter
2. Ex√©cution du workflow de traitement
3. G√©n√©ration de contenu bas√© sur la base de connaissances
4. Validation et pr√©paration pour publication
5. G√©n√©ration du rapport quotidien
"""
import os
import asyncio
import json
import aiofiles
import logging
from dataclasses import asdict, is_dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Any

from dotenv import load_dotenv

# Configuration du logging
log_file = "daily_workflow.log"
try:
    # Essayer de supprimer le fichier de log s'il existe
    if os.path.exists(log_file):
        try:
            os.remove(log_file)
        except PermissionError:
            # Si le fichier est verrouill√©, ajouter un horodatage au nom du fichier
            import time
            log_file = f"daily_workflow_{int(time.time())}.log"
            
    # Configurer le logging avec le nouveau nom de fichier
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
except Exception as e:
    # En cas d'erreur, utiliser uniquement la console
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    logging.error(f"Impossible de configurer le fichier de log: {e}. Utilisation de la console uniquement.")
logger = logging.getLogger(__name__)

# Chargement des variables d'environnement
load_dotenv()

# Configuration des chemins
BASE_DIR = Path(__file__).resolve().parent
INPUT_DIR = BASE_DIR / "input"
OUTPUT_DIR = BASE_DIR / "output"
REPORTS_DIR = OUTPUT_DIR / "reports"
CONTENT_DIR = OUTPUT_DIR / "content"

# Cr√©ation des dossiers n√©cessaires
for directory in [INPUT_DIR, OUTPUT_DIR, REPORTS_DIR, CONTENT_DIR]:
    directory.mkdir(exist_ok=True, parents=True)


class CustomEncoder(json.JSONEncoder):
    """Encodeur JSON personnalis√© pour g√©rer les dataclasses, enums et Path."""
    def default(self, o: Any) -> Any:
        if is_dataclass(o):
            return asdict(o)
        if isinstance(o, Enum):
            return o.value
        if isinstance(o, Path):
            return str(o)
        return super().default(o)


class DailyWorkflow:
    """Classe principale pour g√©rer le workflow quotidien."""

    def __init__(self):
        """Initialise le contexte et les agents."""
        self.context = {
            "date_execution": datetime.now().strftime("%Y-%m-%d"),
            "statistiques": {
                "pdf_traites": 0,
                "txt_traites": 0,
                "articles_generes": 0,
                "publications_sociales": 0,
                "visuels_crees": 0,
                "erreurs": [],
            },
            "contenus_generes": {
                "articles": [],
                "publications": [],
                "visuels": [],
                "fichiers": [],
            }
        }
        self.agents = {}
        logger.info("Workflow initialis√©.")

    async def initialize_agents(self):
        """Initialise tous les agents n√©cessaires de mani√®re asynchrone."""
        logger.info("Initialisation des agents...")
        from agents.pdf_analyzer import PDFAnalyzerAgent
        from agents.theme_manager import ThemeManagerAgent
        from agents.content_strategy import ContentStrategyAgent
        from agents.blog_writer import BlogWriterAgent
        from agents.social_creator import SocialCreatorAgent
        from agents.visual_creator import VisualCreatorAgent
        from agents.validator import ValidatorAgent

        self.agents = {
            'pdf_analyzer': PDFAnalyzerAgent(),
            'theme_manager': ThemeManagerAgent(),
            'content_strategy': ContentStrategyAgent(),
            'blog_writer': BlogWriterAgent(),
            'social_creator': SocialCreatorAgent(),
            'visual_creator': VisualCreatorAgent(),
            'validator': ValidatorAgent()
        }
        logger.info("Tous les agents ont √©t√© initialis√©s.")

    async def check_for_new_files(self) -> List[Path]:
        """V√©rifie la pr√©sence de nouveaux fichiers PDF ou TXT √† traiter."""
        logger.info(f"Recherche de nouveaux fichiers dans : {INPUT_DIR}")
        pdf_files = list(INPUT_DIR.glob("*.pdf"))
        txt_files = list(INPUT_DIR.glob("*.txt"))
        all_files = pdf_files + txt_files
        if not all_files:
            logger.info("Aucun nouveau fichier (PDF/TXT) d√©tect√©.")
        else:
            logger.info(f"{len(all_files)} nouveau(x) fichier(s) d√©tect√©(s): {', '.join(f.name for f in all_files)}")
        return all_files

    async def process_file(self, file_path: Path) -> Dict:
        """Traite un seul fichier (PDF ou TXT) via le pipeline d'agents."""
        logger.info(f"--- D√©but du traitement du fichier : {file_path.name} ---")
        try:
            # 1. Analyse du fichier
            if file_path.suffix.lower() == '.pdf':
                analysis_result = await self.agents['pdf_analyzer'].analyze_pdf(str(file_path))
                content = analysis_result.get('resume', '')
                self.context['statistiques']['pdf_traites'] += 1
            elif file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                analysis_result = {'resume': content, 'metadata': {'source': file_path.name}}
                self.context['statistiques']['txt_traites'] += 1
            else:
                raise ValueError(f"Format de fichier non support√© : {file_path.suffix}")

            # 2. Analyse th√©matique
            theme_analysis = await self.agents['theme_manager'].analyze_content(content=content, content_type="pdf_analysis")

            # 3. Strat√©gie de contenu
            content_strategy = await self.agents['content_strategy'].generate_strategy(analysis=analysis_result, theme_analysis=theme_analysis)

            # 4. R√©daction de l'article de blog
            topic = content_strategy.get('suggested_topics', ['M√©decine Traditionnelle Chinoise'])[0]
            blog_post_dict = await self.agents['blog_writer'].write_article(topic=topic, target_audience="grand public", style="professionnel")
            
            # V√©rifier si la g√©n√©ration a r√©ussi
            if blog_post_dict.get('status') == 'success':
                # Extraire le contenu de l'article et les m√©tadonn√©es
                article_content = blog_post_dict.get('article', '')
                metadata = blog_post_dict.get('metadata', {})
                
                # Cr√©er un dictionnaire avec le contenu et les m√©tadonn√©es
                blog_post = {
                    'content': article_content,
                    'metadata': metadata
                }
                
                self.context['contenus_generes']['articles'].append(blog_post)
                self.context['statistiques']['articles_generes'] += 1
            else:
                error_msg = blog_post_dict.get('message', 'Erreur inconnue lors de la g√©n√©ration de l\'article')
                logger.error(f"√âchec de la g√©n√©ration de l'article: {error_msg}")
                blog_post_dict = {'error': error_msg}

            # 5. Validation du contenu
            validation_result = await self.agents['validator'].validate_content(content=blog_post_dict, content_type="article_blog")

            # 6. Cr√©ation des publications pour les r√©seaux sociaux
            social_posts = await self.agents['social_creator'].create_posts(content_data=blog_post_dict)
            self.context['contenus_generes']['publications'].extend(social_posts)
            self.context['statistiques']['publications_sociales'] += len(social_posts)

            # 7. Cr√©ation des visuels
            visual_elements = []
            for post in social_posts:
                prompt_input = {
                    'type_visuel': post.get('type_visuel', 'citation'),
                    'theme': post.get('theme', 'MTC'),
                    'elements': [post.get('content', '')],
                    'style': 'moderne'
                }
                visual = self.agents['visual_creator'].generate_prompt(prompt_input)
                visual_elements.append(visual)
            self.context['contenus_generes']['visuels'].extend(visual_elements)
            self.context['statistiques']['visuels_crees'] += len(visual_elements)

            # 8. Assemblage des r√©sultats
            result = {
                'source_file': file_path.name,
                'analysis': analysis_result,
                'theme_analysis': theme_analysis,
                'content_strategy': content_strategy,
                'blog_post': blog_post_dict.get('article', '') if isinstance(blog_post_dict, dict) else str(blog_post_dict),
                'blog_metadata': blog_post_dict.get('metadata', {}) if isinstance(blog_post_dict, dict) else {},
                'validation': validation_result,
                'social_posts': social_posts,
                'visual_elements': visual_elements,
            }

            # 9. Sauvegarde des r√©sultats
            await self.save_results(result, file_path.stem)
            logger.info(f"--- Fin du traitement du fichier : {file_path.name} ---")
            return result

        except Exception as e:
            logger.error(f"Erreur lors du traitement du fichier {file_path.name}: {e}", exc_info=True)
            self.context['statistiques']['erreurs'].append({'fichier': file_path.name, 'erreur': str(e)})
            return {}

    async def save_results(self, result: Dict, base_filename: str):
        """Sauvegarde les r√©sultats du traitement dans des fichiers JSON et Markdown."""
        output_dir = CONTENT_DIR / base_filename
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Sauvegarde des r√©sultats dans : {output_dir}")

        try:
            # Sauvegarde du r√©sultat complet
            result_file = output_dir / 'resultat_complet.json'
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, cls=CustomEncoder)

            # Sauvegarde de l'article de blog
            blog_post = result.get('blog_post')
            if blog_post:
                blog_file = output_dir / 'article_blog.md'
                content = blog_post.get('content', '') if isinstance(blog_post, dict) else str(blog_post)
                with open(blog_file, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            logger.info(f"R√©sultats pour '{base_filename}' sauvegard√©s avec succ√®s.")

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des fichiers pour {base_filename}: {e}", exc_info=True)
            # Ne propagez pas l'erreur pour ne pas bloquer le reste du workflow
            self.context['statistiques']['erreurs'].append({'fichier': base_filename, 'erreur': f"Sauvegarde √©chou√©e: {e}"})


    async def generate_daily_report(self):
        """G√©n√®re le rapport quotidien de production en JSON et Markdown."""
        logger.info("G√©n√©ration du rapport quotidien...")
        try:
            report_data = {
                'date': self.context['date_execution'],
                'statistiques': self.context['statistiques'],
                'resume_contenu': {
                    'articles': [
                        {'titre': a.get('metadata', {}).get('title', 'Titre inconnu'), 'mots': len(a.get('content', '').split())}
                        for a in self.context['contenus_generes']['articles'] if isinstance(a, dict)
                    ],
                    'publications': len(self.context['contenus_generes']['publications']),
                    'visuels': len(self.context['contenus_generes']['visuels'])
                },
                'erreurs': self.context['statistiques']['erreurs']
            }

            # Sauvegarde du rapport JSON
            report_file_json = REPORTS_DIR / f"rapport_{self.context['date_execution']}.json"
            with open(report_file_json, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, cls=CustomEncoder)

            # G√©n√©ration et sauvegarde du rapport Markdown
            report_file_md = REPORTS_DIR / f"rapport_{self.context['date_execution']}.md"
            await self._generate_human_readable_report(report_data, report_file_md)

            logger.info(f"Rapport quotidien g√©n√©r√© : {report_file_json.name} et {report_file_md.name}")
            return report_data
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du rapport : {e}", exc_info=True)
            return {}

    async def _generate_human_readable_report(self, report_data: Dict, output_path: Path):
        """G√©n√®re une version lisible du rapport en Markdown."""
        stats = report_data['statistiques']
        contenus = report_data['resume_contenu']
        
        lines = [
            f"# RAPPORT QUOTIDIEN - {report_data['date']}",
            "## üìä Statistiques de production",
            f"- üìÑ Fichiers trait√©s: {stats.get('pdf_traites', 0) + stats.get('txt_traites', 0)}",
            f"- üìù Articles g√©n√©r√©s: {stats.get('articles_generes', 0)}",
            f"- üì± Publications sociales: {stats.get('publications_sociales', 0)}",
            f"- üé® Visuels cr√©√©s: {stats.get('visuels_crees', 0)}",
            f"- ‚ùå Erreurs: {len(stats.get('erreurs', []))}",
            "\n## üìù Contenus g√©n√©r√©s",
            "### Articles de blog"
        ]
        
        articles = contenus.get('articles', [])
        if articles:
            for article in articles:
                lines.append(f"- {article.get('titre', 'Sans titre')} ({article.get('mots', 0)} mots)")
        else:
            lines.append("- Aucun article g√©n√©r√©.")

        if stats.get('erreurs'):
            lines.append("\n## ‚ùå Erreurs rencontr√©es")
            for error in stats['erreurs']:
                lines.append(f"- **Fichier/√âtape:** {error.get('fichier', 'N/A')} - **Erreur:** {error.get('erreur', 'Inconnue')}")
        
        # Utilise aiofiles pour l'√©criture asynchrone
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write("\n".join(lines))
    
    async def run(self, max_concurrent_tasks: int = 3):
        """
        Ex√©cute le workflow quotidien complet.
        
        Args:
            max_concurrent_tasks: Nombre maximum de t√¢ches √† ex√©cuter en parall√®le
        """
        logger.info("=== D√âMARRAGE DU WORKFLOW QUOTIDIEN ===")
        start_time = datetime.now()
        
        try:
            # Initialisation des agents
            await self.initialize_agents()
            
            # V√©rification des nouveaux fichiers (PDF ou TXT)
            input_files = await self.check_for_new_files()
            
            if input_files:
                logger.info(f"Traitement de {len(input_files)} fichiers en parall√®le (max {max_concurrent_tasks} t√¢ches simultan√©es)")
                
                # Cr√©ation d'une s√©maphore pour limiter le nombre de t√¢ches concurrentes
                semaphore = asyncio.Semaphore(max_concurrent_tasks)
                
                async def process_with_semaphore(file_path):
                    async with semaphore:
                        try:
                            return await self.process_file(file_path)
                        except Exception as e:
                            logger.error(f"Erreur lors du traitement de {file_path.name}: {str(e)}", exc_info=True)
                            return {"status": "error", "file": str(file_path), "error": str(e)}
                
                # Cr√©ation des t√¢ches de traitement
                tasks = [process_with_semaphore(file_path) for file_path in input_files]
                
                # Ex√©cution des t√¢ches en parall√®le avec gestion des erreurs
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Traitement des r√©sultats
                for result in results:
                    if isinstance(result, Exception):
                        logger.error(f"Erreur dans une t√¢che de traitement: {str(result)}")
                    elif isinstance(result, dict) and result.get('status') == 'error':
                        logger.error(f"Erreur de traitement: {result.get('error')}")
            else:
                logger.info("Aucun fichier √† traiter, g√©n√©ration de contenu √† partir de la base de connaissances")
                await self.generate_content_from_knowledge()
            
            # G√©n√©ration du rapport quotidien
            await self.generate_daily_report()
            
            # Calcul du temps d'ex√©cution
            duration = (datetime.now() - start_time).total_seconds() / 60
            logger.info(f"=== WORKFLOW TERMIN√â EN {duration:.1f} MINUTES ===")
            
            return True
            
        except Exception as e:
            logger.error(f"ERREUR LORS DE L'EX√âCUTION DU WORKFLOW: {str(e)}", exc_info=True)
            return False


async def main():
    """Fonction principale."""
    workflow = DailyWorkflow()
    success = await workflow.run()
    return 0 if success else 1


if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))
